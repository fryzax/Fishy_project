# ===============================================
# Script : train_model.py
# Objectif : Entra√Æner un mod√®le CNN pour classifier les poissons
#            + Tracking des exp√©riences avec MLflow
# Auteur : Mathieu + Kirsten
# ===============================================

import os
import io
import pymysql
from minio import Minio
from PIL import Image
from tqdm import tqdm
import torch
from torch import nn, optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
from urllib.parse import urljoin
import shutil
import time

# === üîπ MLflow ===
import mlflow
import mlflow.pytorch  # si tu veux log ton mod√®le torch

# ============================================================
# 1Ô∏è‚É£ Configuration
# ============================================================

# D√©tection de l'environnement (local ou Docker)
# Si on tourne en local, on utilise localhost, sinon les noms de services Docker
import socket

def is_running_in_docker():
    """D√©tecte si le script tourne dans un conteneur Docker"""
    try:
        with open('/proc/1/cgroup', 'r') as f:
            return 'docker' in f.read()
    except:
        return False

IN_DOCKER = is_running_in_docker()

# MinIO configuration
MINIO_ENDPOINT = "minio:9000"
MINIO_ACCESS_KEY = "admin-user"
MINIO_SECRET_KEY = "admin-password"
BUCKET_NAME = "dataset-fish"
MODEL_BUCKET = "models"

# MySQL configuration
MYSQL_HOST = "mysql" if IN_DOCKER else "localhost"
MYSQL_USER = "root"
MYSQL_PASSWORD = "root"
MYSQL_DB = "mlops"

# Dossiers locaux
DATA_DIR = "data"
TRAIN_DIR = os.path.join(DATA_DIR, "train")

# Param√®tres d'entra√Ænement
EPOCHS = 20
BATCH_SIZE = 16
LEARNING_RATE = 0.001
IMG_SIZE = 224

print(f"üñ•Ô∏è  Environnement d√©tect√©: {'Docker' if IN_DOCKER else 'Local'}")

# ============================================================
# 2Ô∏è‚É£ Connexion MySQL, MinIO et MLflow
# ============================================================
print("üîå Connexion √† MySQL...")
conn = pymysql.connect(
    host=MYSQL_HOST,
    user=MYSQL_USER,
    password=MYSQL_PASSWORD,
    database=MYSQL_DB
)
cursor = conn.cursor()
print("‚úÖ Connect√© √† MySQL")

print("üîå Connexion √† MinIO...")
minio_client = Minio(
    MINIO_ENDPOINT,
    access_key=MINIO_ACCESS_KEY,
    secret_key=MINIO_SECRET_KEY,
    secure=False
)

# V√©rification des buckets
if not minio_client.bucket_exists(BUCKET_NAME):
    raise ValueError(f"Le bucket '{BUCKET_NAME}' n'existe pas sur MinIO.")
if not minio_client.bucket_exists(MODEL_BUCKET):
    minio_client.make_bucket(MODEL_BUCKET)
    print(f"‚úÖ Bucket '{MODEL_BUCKET}' cr√©√© pour stocker les mod√®les.")

# Cr√©er le bucket mlflow pour les artifacts MLflow
MLFLOW_BUCKET = "mlflow"
if not minio_client.bucket_exists(MLFLOW_BUCKET):
    minio_client.make_bucket(MLFLOW_BUCKET)
    print(f"‚úÖ Bucket '{MLFLOW_BUCKET}' cr√©√© pour les artifacts MLflow.")

print("‚úÖ Connect√© √† MinIO")

# ============================================================
# Configuration des credentials S3/MinIO pour MLflow
# ============================================================
print("üîå Configuration des credentials MLflow pour MinIO...")
os.environ["AWS_ACCESS_KEY_ID"] = MINIO_ACCESS_KEY
os.environ["AWS_SECRET_ACCESS_KEY"] = MINIO_SECRET_KEY
os.environ["MLFLOW_S3_ENDPOINT_URL"] = f"http://minio:9000" if IN_DOCKER else "http://localhost:9000"
print("‚úÖ Credentials configur√©s pour MLflow")

print("üîå Configuration de MLflow...")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
print(f"‚úÖ MLflow configur√© : {MLFLOW_TRACKING_URI}")

# ============================================================
# 3Ô∏è‚É£ Lecture des images depuis la table SQL
# ============================================================
print("üìä Lecture des donn√©es depuis la table fish_data...")
cursor.execute("SELECT species_label, file_name, url_s3 FROM fish_data WHERE split = 'train'")
rows = cursor.fetchall()
print(f"‚úÖ {len(rows)} images de training trouv√©es dans la base de donn√©es")

# ============================================================
# 4Ô∏è‚É£ T√©l√©chargement des images depuis MinIO
# ============================================================
print("üì¶ T√©l√©chargement des images depuis MinIO...")
os.makedirs(TRAIN_DIR, exist_ok=True)

for label, file_name, url_s3 in tqdm(rows, desc="Downloading images"):
    key = f"train/{label}/{file_name}"
    label_dir = os.path.join(TRAIN_DIR, label)
    os.makedirs(label_dir, exist_ok=True)
    local_path = os.path.join(label_dir, file_name)
    if not os.path.exists(local_path):
        try:
            minio_client.fget_object(BUCKET_NAME, key, local_path)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur pour {key}: {e}")

cursor.close()
conn.close()
print("‚úÖ T√©l√©chargement termin√©.")

# ============================================================
# 5Ô∏è‚É£ Pr√©paration des donn√©es PyTorch
# ============================================================
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
print(f"üìä Dataset pr√™t : {len(train_dataset)} train / {len(val_dataset)} val images")

# ============================================================
# 6Ô∏è‚É£ Construction du mod√®le CNN
# ============================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet18(weights="IMAGENET1K_V1")
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, len(dataset.classes))
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# ============================================================
# 7Ô∏è‚É£ Int√©gration MLflow
# ============================================================
mlflow.set_tracking_uri("http://mlflow:5000")  # ton service docker-compose
mlflow.set_experiment("Fishy_Model_Tracking")

with mlflow.start_run(run_name=f"FishClassifier_{int(time.time())}"):

    # Log des hyperparam√®tres
    mlflow.log_param("epochs", EPOCHS)
    mlflow.log_param("batch_size", BATCH_SIZE)
    mlflow.log_param("learning_rate", LEARNING_RATE)
    mlflow.log_param("img_size", IMG_SIZE)
    mlflow.log_param("architecture", "resnet18")

    print("üöÄ D√©but de l'entra√Ænement avec MLflow tracking...")
    best_val_acc = 0.0
    best_model_path = "best_model.pt"

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]", leave=False):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_loss = running_loss / len(train_loader)

        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]", leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_loss /= len(val_loader)
        val_accuracy = 100 * correct / total

        # Log des m√©triques
        mlflow.log_metric("train_loss", train_loss, step=epoch)
        mlflow.log_metric("val_loss", val_loss, step=epoch)
        mlflow.log_metric("val_accuracy", val_accuracy, step=epoch)

        print(f"üìà Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%")

        if val_accuracy > best_val_acc:
            best_val_acc = val_accuracy
            torch.save(model.state_dict(), best_model_path)
            print(f"   ‚≠ê Nouveau meilleur mod√®le sauvegard√© (Val Acc: {val_accuracy:.2f}%)")

    # Fin d'entra√Ænement
    print(f"‚úÖ Entra√Ænement termin√©. Meilleure pr√©cision validation : {best_val_acc:.2f}%")
    mlflow.log_metric("best_val_accuracy", best_val_acc)

    # Log du mod√®le final
    MODEL_PATH = "model_v1.pt"
    shutil.copy(best_model_path, MODEL_PATH)
    mlflow.pytorch.log_model(model, "model")  # log du mod√®le dans MLflow

# ============================================================
# 8Ô∏è‚É£ Upload du mod√®le vers MinIO
# ============================================================
try:
    timestamp = int(time.time())
    model_name = f"model_v1_{timestamp}.pt"
    with open(MODEL_PATH, 'rb') as file_data:
        file_stat = os.stat(MODEL_PATH)
        minio_client.put_object(MODEL_BUCKET, model_name, file_data, file_stat.st_size)
    print(f"‚úÖ Mod√®le envoy√© sur MinIO : bucket='{MODEL_BUCKET}', objet='{model_name}'")
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur upload vers MinIO : {e}")

print("üéâ Entra√Ænement + tracking MLflow termin√© avec succ√®s !")
